{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create a batch web service for a Spark model on Azure\n",
    "\n",
    "Before running the tutorial, you must configure your DSVM as specified in the README on the [Deploying Spark ML Models on Azure (Preview)](https://github.com/Azure/Spark-Operationalization-On-Azure) GitHub repo. If you have previously configured your DSVM, you may want to check the GitHub repo to ensure that you are using the most recent instructions.\n",
    "\n",
    "\n",
    "In the tutorial you will use [Apache Spark](http://spark.apache.org/) to create a model that uses a Logistic Regression learner to predict food inspection results. To do this, you will call the Spark Python API ([PySpark](http://spark.apache.org/docs/0.9.0/python-programming-guide.html)) to load a dataset, train a model using the dataset, and publish a batch scoring API for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create a batch web service for a Spark model on Azure\n",
    "\n",
    "Before running the tutorial, you must configure your DSVM as specified in the README on the [Operationalizing Spark Models on Azure](https://github.com/Azure/Spark-Operationalization-On-Azure) GitHub repo.\n",
    "\n",
    "In the tutorial you will use [Apache Spark](http://spark.apache.org/) to create a model that uses a Logistic Regression learner to predict food inspection results. To do this, you will call the Spark Python API ([PySpark](http://spark.apache.org/docs/0.9.0/python-programming-guide.html)) to load a dataset, train a model using the dataset, and publish a batch scoring API for the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "The tutorial uses the *Food Inspections Data Set* which contains the results of food inspections that were conducted in Chicago. To facilitate this tutorial, we have placed a copy of the data in the ```azureml/datasets``` folder. The original dataset is available from the [City of Chicago data portal](https://data.cityofchicago.org/Health-Human-Services/Food-Inspections/4ijn-s7e5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Import the relevant PySpark bindings\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the food inspections dataset and create numerical labels for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inspections = spark.read.csv(\"../datasets/food_inspections1.csv\",mode='DROPMALFORMED',inferSchema=False)\n",
    "\n",
    "schema = StructType([StructField(\"id\", IntegerType(), False), \n",
    "                     StructField(\"name\", StringType(), False), \n",
    "                     StructField(\"results\", StringType(), False), \n",
    "                     StructField(\"violations\", StringType(), True)])\n",
    "\n",
    "df = sqlContext.createDataFrame(inspections.rdd.map(lambda l: (int(l[0]), l[1], l[12], l[13] if l[13] else '')), schema) \n",
    "df.registerTempTable('CountResults')\n",
    "\n",
    "def labelForResults(s):\n",
    "    if s == 'Fail':\n",
    "        return 0.0\n",
    "    elif s == 'Pass w/ Conditions' or s == 'Pass':\n",
    "        return 1.0\n",
    "    else:\n",
    "        return -1.0\n",
    "    \n",
    "label = UserDefinedFunction(labelForResults, DoubleType())\n",
    "labeledData = df.select(label(df.results).alias('label'), df.violations).where('label >= 0')\n",
    "labeledData.write.format('parquet').mode('overwrite').save('foo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and save the model\n",
    "Next, you train a logistic regression model to predict inspection results. The following code tokenizes each \"violations\" string to get the individual words in each string. It then uses a HashingTF to convert each set of tokens into a feature vector which is passed to the logistic regression algorithm to construct a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"violations\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "model = pipeline.fit(labeledData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you save the model to use when deploying the web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.write().overwrite().save(\"food_inspection.model\")\n",
    "print \"Model saved\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Batch Web Service\n",
    "\n",
    "In this section, you will create and deploy a batch webservice that will make predictions on given data using the model that you trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a prediction script \n",
    "\n",
    "Your goal is to create an endpoint that you can call to make predictions based on the input data. To create a web service using the model you saved, you start by authoring a script to do the scoring.\n",
    "\n",
    "In the provided sample, the script takes a data file as its input-data argument, uses the model specified by the user as model input, and makes predictions on the data by running the model. The script then saves the predictions as a parquet file to the path provided through the output-data argument.\n",
    "\n",
    "The save file call (```%%save_file -f batch_score.py```) in the first line of the of the cell saves the contents of the cell to a local file with the name supplied by the ```-f``` argument.\n",
    "\n",
    "When calling the batch web service create command for the scoring script using the AML CLI, you must provide the parameters that you identified in the script as command line arguments.\n",
    "\n",
    "You can choose to parameterize your scoring script files per your descretion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%save_file -f batch_score.py\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import *\n",
    "import argparse\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext.getOrCreate(sc)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input-data\")\n",
    "parser.add_argument(\"--trained-model\")\n",
    "parser.add_argument(\"--output-data\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "print str(args.input_data)\n",
    "print str(args.trained_model)\n",
    "print str(args.output_data)\n",
    "\n",
    "testdata = spark.read.csv(str(args.input_data),mode='DROPMALFORMED',inferSchema=False)\n",
    "\n",
    "schema = StructType([StructField(\"id\", IntegerType(), False), \n",
    "                     StructField(\"name\", StringType(), False), \n",
    "                     StructField(\"results\", StringType(), False), \n",
    "                     StructField(\"violations\", StringType(), True)])\n",
    "\n",
    "testDf = sqlContext.createDataFrame(testdata.rdd.map(lambda l: (int(l[0]), l[1], l[12], l[13] if l[13] else '')), schema).where(\"results = 'Fail' OR results = 'Pass' OR results = 'Pass w/ Conditions'\")\n",
    "\n",
    "model = PipelineModel.load(args.trained_model)\n",
    "\n",
    "predictionsDf = model.transform(testDf)\n",
    "predictionsDf.write.format(\"parquet\").mode(\"overwrite\").save(str(args.output_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the CLI to deploy and manage your batch web service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open an SSH session to your DSVM and change to the folder notebooks/azureml/batch.\n",
    "```\n",
    "cd ~/notebooks/azureml/batch\n",
    "```\n",
    "\n",
    "#### Deploy to local VM\n",
    "\n",
    "To create the batch web service locally on the DSVM, set your CLI environment to run in local mode.\n",
    "```\n",
    "aml env local\n",
    "```\n",
    "\n",
    "To create the web service, run the following command:\n",
    "\n",
    "```\n",
    "aml service create batch -f batch_score.py -n batchwebservice -i --input-data -i --trained-model=food_inspection.model -o --output-data=food_inspection_predictions.parquet\n",
    "```\n",
    "\n",
    "Once the web service is successfully created, the following command runs a job against the web service:\n",
    "\n",
    "```\n",
    "aml service run batch -n batchwebservice -w -i --input-data=../datasets/food_inspections2.csv \n",
    "```\n",
    "The run command provides a dataset as input and executes with remaining default parameters as specified during web service creation. You may also choose to speciy your own parameters.\n",
    "\n",
    "The **-w** parameter specifies that the job is run synchronously. If you do not specify **-w**, you can view the status of the job using the following commands:\n",
    "\n",
    "View the list of jobs running against your web service to get the ID of the job:\n",
    "\n",
    "```\n",
    "aml service listjobs batch -n batchwebservice\n",
    "```\n",
    "Use the Job Name to view the status with the following command:\n",
    "```\n",
    "aml service viewjob batch -n batchwebservice -j <paste job name here>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy to HDInsight Cluster\n",
    "\n",
    "If you haven't already, provision an Azure HDInsight Spark 2.0 cluster. \n",
    "\n",
    "To provision an HDInsight Spark 2.0 cluster:\n",
    "\n",
    "1. Sign in to the the [Azure portal](https://portal.azure.com).\n",
    "2. click **New** and type HDInsight\n",
    "3. Select HDInsight from the list returned results.\n",
    "4. Click **Create**.\n",
    "5. Enter a **Cluster name**.\n",
    "6. Click **Cluster configuration**.\n",
    "\t1. In **Cluster type**, select **Spark**. \n",
    "\t2. in **Version**, select **Spark 2.0.1 (HDI 3.5)**.\n",
    "\t3. Click **Select**.\n",
    "7. Click **Credentials** and configure the credentials for the cluster. To access Jupyter notebooks, the SSH user name must be all lower case and you must select password authentication.\n",
    "8. Click **Cluster size**, then click **Select** to accept the default options.\n",
    "9. Select a resource group to to contain the cluster.\n",
    "10. Click **Create**.\n",
    "\n",
    "After the cluster deployment is complete must install the Azure Machine Learning Batch application to enable the cluster to execute on the commands submitted through the Azure ML CLI from your local machine.\n",
    "\n",
    "To install the AMLBatch app on your HDInsight cluster, click the following link: \n",
    "\n",
    "<a href=\"https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fazuremlbatchtest.blob.core.windows.net%2Ftemplates%2FinstallTemplate.json\" target=\"_blank\">AML Batch Install Template</a>\n",
    "\n",
    "When the template opens, provide the Resource Group and name of the HDInsight Cluster where the web service will be deployed. Leave the node size and count fields as is. Accept the license agreement and click **purchase**.\n",
    "\n",
    "Open an SSH session to your DSVM and change to the folder notebooks/azureml/batch.\n",
    "Set your CLI environment to run in cluster mode.\n",
    "\n",
    "```\n",
    "aml env cluster\n",
    "```\n",
    "You will see the below prompt:\n",
    "```\n",
    "Would you like to set up port forwarding to your ACS cluster (Y/n)? n\n",
    "```\n",
    "Type **n** at the above prompt since you are not running the RRS scenario on ACS.\n",
    "\n",
    "Type **y** to continue with cluster mode at the below prompt:\n",
    "\n",
    "```\n",
    "Could not connect to ACS cluster. Continue with cluster mode anyway (y/N)? y\n",
    "```\n",
    "To target the HDInsight Cluster and associated storage, type the below commands on the terminal to set the following environment variables for successful creation of the web service\n",
    "```\n",
    "\texport AML_STORAGE_ACCT_NAME=<your storage account name>\n",
    "\texport AML_STORAGE_ACCT_KEY=<your storage account key>\n",
    "\texport AML_HDI_CLUSTER=<the url to your hdinsight cluster>\n",
    "\texport AML_HDI_USER=<your hdinsight user name>\n",
    "\texport AML_HDI_PW=<your hdinsight user password>\n",
    "```\n",
    "**Important**: Make sure the storage account you use is the one that's associated with your HDInsight Cluster.\n",
    "\n",
    "To create the web service on the HDInsight cluster, run the following command:\n",
    "\n",
    "```\n",
    "aml service create batch -f batch_score.py -n batchwebservice -i --input-data -i --trained-model=food_inspection.model -o --output-data=wasb:///HdiSamples/HdiSamples/food_inspection_predictions.parquet\n",
    "```\n",
    "In the above command, we are specifying the output to be written in the cluster.  \n",
    "\n",
    "Once the web service is successfully created, the following command runs a job against the web service:\n",
    "\n",
    "```\n",
    "aml service run batch -n batchwebservice -w -i --input-data=../datasets/food_inspections2.csv \n",
    "```\n",
    "\n",
    "This run command provides a dataset as input and executes with remaining default parameters as specified during web service creation. You can also choose to speciy your own parameters.\n",
    "\n",
    "The **-w** parameter specifies that the job is run synchronously. If you do not specify **-w**, you can view the status of the job using the following commands:\n",
    "\n",
    "View the list of jobs running against your web service to get the ID of the job:\n",
    "\n",
    "```\n",
    "aml service listjobs batch -n batchwebservice\n",
    "```\n",
    "Use the Job Name to view the status with the following command:\n",
    "```\n",
    "aml service viewjob batch -n batchwebservice -j <paste job name here>\n",
    "```\n",
    "---  \n",
    "Created by a Microsoft Employee.  \n",
    "Copyright (C) Microsoft. All Rights Reserved."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Spark - python",
   "language": "python",
   "name": "spark-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
