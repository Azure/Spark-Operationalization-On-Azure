{"nbformat_minor": 1, "cells": [{"source": "## Building a sample AzureML Spark Batch web service\n\nIn this tutorial you will use [Apache Spark](http://spark.apache.org/) to create a model that uses a Logistic Regression learner to predict food inspection results. To do this, you will call the Spark Python API ([PySpark](http://spark.apache.org/docs/0.9.0/python-programming-guide.html)) to load a dataset, train a model using the dataset, and publish a batch scoring API for the model.", "cell_type": "markdown", "metadata": {}}, {"source": "### Load the data\n\nThe tutorial uses the *Food Inspections Data Set* which contains the results of food inspections that were conducted in Chicago. To facilitate this tutorial, we have placed a copy of the data in the ```azureml/datasets``` folder. The original dataset is available from the [City of Chicago data portal](https://data.cityofchicago.org/). ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "### Import the relevant PySpark bindings\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import HashingTF, Tokenizer\nfrom pyspark.sql.functions import UserDefinedFunction\nfrom pyspark.sql.types import *", "outputs": [], "metadata": {"collapsed": true}}, {"source": "#### Read in the food inspections dataset and create numerical labels for training", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "def csvParse(s):\n    import csv\n    from StringIO import StringIO\n    sio = StringIO(s)\n    value = csv.reader(sio).next()\n    sio.close()\n    return value\n\ninspections = sc.textFile(str(\"datasets/food_inspections1.csv\")).map(csvParse)\n\nschema = StructType([StructField(\"id\", IntegerType(), False), \n                     StructField(\"name\", StringType(), False), \n                     StructField(\"results\", StringType(), False), \n                     StructField(\"violations\", StringType(), True)])\n\ndf = sqlContext.createDataFrame(inspections.map(lambda l: (int(l[0]), l[1], l[12], l[13])) , schema)\ndf.registerTempTable('CountResults')\n\ndef labelForResults(s):\n    if s == 'Fail':\n        return 0.0\n    elif s == 'Pass w/ Conditions' or s == 'Pass':\n        return 1.0\n    else:\n        return -1.0\n    \nlabel = UserDefinedFunction(labelForResults, DoubleType())\nlabeledData = df.select(label(df.results).alias('label'), df.violations).where('label >= 0')", "outputs": [], "metadata": {"collapsed": false}}, {"source": "#### Create and save the model\nNext, you train a logistic regression model to predict inspection results. The following code tokenizes each \"violations\" string to get the individual words in each string. It then uses a HashingTF to convert each set of tokens into a feature vector which is passed to the logistic regression algorithm to construct a model. ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "tokenizer = Tokenizer(inputCol=\"violations\", outputCol=\"words\")\nhashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\nlr = LogisticRegression(maxIter=10, regParam=0.01)\npipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n\nmodel = pipeline.fit(labeledData)", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Finally, you save the model to use when deploying the web service.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": "model.write().overwrite().save(\"food_inspection.model\")\nprint \"Model saved\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Model saved\n"}], "metadata": {"collapsed": false}}, {"source": "## Authoring a Batch Web Service\n\nIn this section, you will author a batch web service the model you saved previously to generate your predictions. ", "cell_type": "markdown", "metadata": {}}, {"source": "### Create a PySpark script that defines the web service\n\nTo deploy a web service, you must create a PySpark script that defines the web service. The script specifies how your web service operates: what inputs it expects from the caller of the web service and what outputs it produces. \n\nIn the script, you identify the input parameters you want your web service to consume and the outputs it should produce. \n\nWhen you create your batch web service using the Azure Machine Learning CLI, you provide the parameters that you identified in the script as command line arguments.\n\nIn the sample provided, the script takes a data file as its input-data argument, uses the saved logistic regression model to make predictions on the data, and then saves the predictions as a parquet file to the path provided through the output-data argument.\n\nThe following cell contains the PySpark script that you pass the AML CLI to create the web service. The save file call (```%%save_file -f batch_score.py```) in the first line of the of the cell saves the contents of the cell to a local file with the name supplied by the ```-f``` argument.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": "%%save_file -f batch_score.py\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.ml import Pipeline, PipelineModel\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import HashingTF, Tokenizer\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import UserDefinedFunction\nfrom pyspark.sql.types import *\nimport argparse\n\nsc = SparkContext.getOrCreate()\nsqlContext = SQLContext.getOrCreate(sc)\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--input-data\")\nparser.add_argument(\"--output-data\")\n\nargs = parser.parse_args()\nprint str(args.input_data)\nprint str(args.output_data)\n\ndef csvParse(s):\n    import csv\n    from StringIO import StringIO\n    sio = StringIO(s)\n    value = csv.reader(sio).next()\n    sio.close()\n    return value\n\nmodel = PipelineModel.load(food_inspection.model)\n\ntestData = sc.textFile(str(args.input_data))\\\n             .map(csvParse) \\\n             .map(lambda l: (int(l[0]), l[1], l[12], l[13]))\n\nschema = StructType([StructField(\"id\", IntegerType(), False), \n                     StructField(\"name\", StringType(), False), \n                     StructField(\"results\", StringType(), False), \n                     StructField(\"violations\", StringType(), True)])\n\ntestDf = sqlContext.createDataFrame(testData, schema).where(\"results = 'Fail' OR results = 'Pass' OR results = 'Pass w/ Conditions'\")\n\npredictionsDf = model.transform(testDf)\n\npredictionsDf.write.parquet(str(args.output_data))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Saved cell to batch_score.py\n"}], "metadata": {"collapsed": false}}, {"source": "### Use the Azure Machine Learning CLI to deploy and manage your batch web service", "cell_type": "markdown", "metadata": {}}, {"source": "#### Deploy to local VM\n\nTo create the batch web service locally on the DSVM, open an SSH session to your DSVM. \n\n**Note**: When you first run the Azure ML CLI you are prompted to configure your Azure ML API key. If you do not have a key, please refer to the readme at [https://github.com/Azure/AzureML-vNext](https://github.com/Azure/AzureML-vNext).\n\nTo deploy the web service, run the following commands:\n\n```\naml env local\naml service create batch -f batch_score.py -n batchwebservice --input=input-data --output=output-data\n```\n\nYou can choose to provide default values for the variables during web service creation. \n\nThe following command is an example of providing the output location during web service creation.\n\n```\naml service create batch -f batch_score.py -n batchwebservice --input=--input-data --output=--output-data:food_inspection_output\n```", "cell_type": "markdown", "metadata": {}}, {"source": "#### Deploy to HDInsight Cluster\nFor instructions to deploy a sample batch web service to your HDInsight Cluster visit the git page: https://github.com/Azure/AzureML-vNext\n\n---  \nCreated by a Microsoft Employee.  \nCopyright (C) Microsoft. All Rights Reserved.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.12", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}