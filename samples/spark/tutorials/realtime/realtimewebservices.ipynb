{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create a real-time web service for a Spark model on Azure\n",
    "\n",
    "Before running the tutorial, you must configure your DSVM as specified in the README on the [Deploying Spark ML Models on Azure (Preview)](https://github.com/Azure/Spark-Operationalization-On-Azure) GitHub repo. If you have previously configured your DSVM, you may want to check the GitHub repo to ensure that you are using the most recent instructions\n",
    "\n",
    "In the tutorial, we will walk you through loading a dataset, exploring\n",
    "its features, training a model on the dataset, and then publishing a\n",
    "realtime scoring API for the model.\n",
    "\n",
    "First, read in the Boston Housing Price dataset. This dataset is publicly available at https://archive.ics.uci.edu/ml/datasets/Housing. We have placed a copy in your ```azureml/datasets``` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the housing price dataset\n",
    "df2 = spark.read.csv(\"../datasets/housing.csv\", header=True, inferSchema=True)\n",
    "df2.show()\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your model\n",
    "\n",
    "Using Spark's ML library, we can train a gradient boosted tree regressor for our data to produce a model that can predict median values of houses in Boston. Once you have trained the model, you can evaluate it for quality using the root mean squared error metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train a boosted decision tree regressor\n",
    "from pyspark.ml.feature import RFormula\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "import numpy as np\n",
    "formula = RFormula(formula=\"MEDV~.\")\n",
    "gbt = GBTRegressor()\n",
    "pipeline = Pipeline(stages=[formula, gbt]).fit(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate scores\n",
    "scores = pipeline.transform(df2)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "print \"R^2 error =\", RegressionEvaluator(metricName=\"r2\").evaluate(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save your model and schema\n",
    "\n",
    "Once you have a model that performs well, you can package it into a scoring service. To prepare for this, save your model and dataset schema locally first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "pipeline.write().overwrite().save(\"housing.model\")\n",
    "print \"Model saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save schema\n",
    "from azuremlcli import azuremlutilities\n",
    "reload(azuremlutilities)\n",
    "azuremlutilities.saveSchema(df2, \"webserviceschema.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authoring a Realtime Web Service\n",
    "\n",
    "In this section, you how author a realtime web service that scores the model you saved above. \n",
    "\n",
    "### Define ```init``` and ```run```\n",
    "\n",
    "Start by defining your ```init``` and ```run``` functions in the cell below. \n",
    "\n",
    "The ```init``` function initializes the web service, loading in any data or models that it needs to score your inputs. In the example below, it loads in the trained model and the schema of your dataset.\n",
    "\n",
    "The ```run``` function defines what is executed on a scoring call. In this simple example, the service loads the json input as a data frame and runs the pipeline on the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%save_file -f testing.py\n",
    "# Prepare the web service definition by authoring\n",
    "# init() and run() functions. Once tested, remove\n",
    "# the commented magic on the first line to save\n",
    "# the cell to a file.\n",
    "def init():\n",
    "    # read in the model file\n",
    "    from pyspark.ml import PipelineModel\n",
    "    global pipeline\n",
    "    pipeline = PipelineModel.load(\"housing.model\")\n",
    "    \n",
    "    # read in the schema\n",
    "    global inputSchema\n",
    "    inputSchema=azuremlutilities.loadSchema(\"webserviceschema.json\")\n",
    "    \n",
    "def run(inputString):\n",
    "    import json\n",
    "    from pyspark.ml import PipelineModel\n",
    "\n",
    "    input=json.loads(inputString)\n",
    "    inputRDD=sc.parallelize(input)\n",
    "    inputDF=spark.createDataFrame(inputRDD,inputSchema, None, False)\n",
    "    score=pipeline.transform(inputDF)\n",
    "    return score.collect()[0]['prediction']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test ```init``` and ```run```\n",
    "\n",
    "Before publishing the web service, you can test the init and run functions in the notebook by running the the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init()\n",
    "run('[[0.00632,18.0,2.31,0,0.538,6.575,65.2,4.09,1,296,15.3,4.98,24.0]]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a script that defines the web service\n",
    "\n",
    "Your goal is to create an endpoint that you can call to make predictions based on the input data. To create a web service using the model you saved, you start by authoring a script to do the scoring.\n",
    " \n",
    "In the script you identify the input parameters you want your web service to consume and the outputs it should produce. \n",
    "\n",
    "Go back to the cell where you defined your ```init``` and ```run``` functions, uncomment the magic in the first line (```#%%save_file -f testing.py```), and run the cell again. This saves the contents of the cell to a local file with the name supplied to the ```-f``` argument.\n",
    "\n",
    "\n",
    "### Use the CLI to deploy and manage your web services\n",
    "\n",
    "SSH into the DSVM and run the following commands to deploy your service locally.\n",
    "\n",
    "Set the environment variables, either from the command line or from a script, that you generated when you setup your DSVM. \n",
    "\n",
    "Change to azureml folder containing the realtime notebook.\n",
    "\n",
    "```\n",
    "cd ~/notebooks/azureml/realtime\n",
    "```\n",
    "Next run the following commands to create the web service:\n",
    "\n",
    "```\n",
    "aml env local\n",
    "aml service create realtime -f testing.py -m housing.model -s webserviceschema.json -n mytestapp\n",
    "```\n",
    "\n",
    "To create and run the web service on the ACS cluster, change to the cluster mode and rerun the service creation command:\n",
    "\n",
    "```\n",
    "aml env cluster\n",
    "aml service create realtime -f testing.py -m housing.model -s webserviceschema.json -n mytestapp\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Spark - python",
   "language": "python",
   "name": "spark-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
